---
tags: mcp
---
### 编写 MCP 服务端脚本

在项目根目录下面创建 `search_mcp.py` 负责注册 “工具” 并通过 stdio 提供服务：

>[!note]
>服务端脚本通过 stdio 和客户端通信的模式是指：
>- 服务端程序从标准输入 (stdin) 读取请求
>- 处理请求并执行相应工具
>- 将结果输出到标志着输出 stdout 
>
>这种模式允许不同编程语言或系统组件通过标准输入输出进行交互，无需网络通信。

```python
# search_mcp.py
from mcp.server.fastmcp import FastMCP
import requests, logging
from openai import OpenAI
from prompts import *

# —— 初始化 FastMCP 服务 —— 
mcp = FastMCP("search_service")

# —— 配置 OpenAI 客户端 —— 
client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=os.getenv("OPENAI_API_KEY"))

# —— 定义工具：网络检索 —— 
@mcp.tool()
def search(query: str) -> str:
    ...
    return aggregated_context

# —— 定义工具：图像描述 —— 
@mcp.tool()
def get_images(query: str) -> dict:
    ...
    return {url: description}

# —— 启动服务 —— 
if __name__ == "__main__":
    mcp.run()
```


- `FastMCP("search_service")` 启动一个 JSON-RPC 风格的调度器，监听 stdin/stdout。
- `@mcp.tool()` 将函数自动注册为可远程调用的“工具”，参数和返回值会被 JSON 序列化，在 stdin/stdout 管道中传输。


### 异步客户端关键点

#### 启动服务子进程

```python
server_params = StdioServerParameters(
	command="python",
	args=[server_script_path],
	env=None
)

stdio_transport = await self.exit_stack.enter_async_context(
	stdio_client(server_params)
)

self.stdio, self.write = stdio_transport
self.session = await self.exit_stack.enter_async_context(
    ClientSession(self.stdio, self.write)
)
await self.session.initialize()
```


- `stdio_client` 用 `subprocess` 启动 `search_mcp.py` 并拿到其 stdin/stdout
- `ClientSession` 在这条管道上线完成“握手”并初始化协议

#### 工具调用循环

注入工具清单给到 LLM 的 system prompt 

每次调用 LLM：

1. 读取它输出的内容

2. 用 `get_clear_json` 检查是否包含工具调用指令（`json ...`）    

3. 解析出 `{ "name": ..., "params": ... }`

4. `await self.session.call_tool(name, params)` 调用 FastMCP 服务

5. 将工具结果再注入到 LLM 的对话上下文

6. 重复上述直到检测到 `finish`

```python
while True:
    flag, json_text = get_clear_json(message.content)
    if flag == 0:
        return plain_llm_answer
    tool_name, tool_args = parse(json_text)
    result = await self.session.call_tool(tool_name, tool_args)
    push_to_messages(tool call + result)
    message = await self.client.chat.completions.create(...)
    if 'finish' in message.content:
        break
```

#### 最终合成回答

收集所有工具调用的文本结果，用 `FINISH_GENERATE` prompt 让 LLM 整合这些结果，生成对用户更友好的输出。











