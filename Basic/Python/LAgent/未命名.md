Lagent 是一个受 pytorch 设计哲学启发的智能体框架。We expect that the analogy of neutral network layers will make the workflow clearer and more intuitive, so users only need to focus on creating layers and defining message passing between them in a Pythonic way. This is a simple tutorial to get you quickly started with building multi-agent applications.

## Basic concepts

### Models as Agents

Agents use `AgentMessage` for communication.

```python
from typing import Dict, List
from lagent.agents import Agent
from lagent.schema import AgentMessage
from lagent.llms import VllmModel, INTERNLM2_META

llm = VllmModel(
	path='Qwen/Qwen2-7B-Instruct',
	meta_template=INTERNLM2_META,
	tp=1,
	top_k=1,
	temperature=1.0,
	stop_words=['<|im_end|>'],
	max_new_tokens=1024,
)

system_prompt = "你的回答只能从“典”，“孝”，“急”三个字中选一个。"
agent = Agent(llm, system_prompt)

user_msg = AgentMessage(sender="user", content="今天的天气情况怎么样")
bot_msg = agent(user_msg)
print(boy_msg)
```

在这里使用的是 vllm 直接启动一个大模型，这样非常费劲，我们选择使用接口的方式引入。

```python
from typing import Dict, List
from lagent.agents import Agent
from lagent.schema import AgentMessage
from lagent.llms import GPTAPI

llm = GPTAPI(
	model_type="Qwen2.5-32B-Instruct",
	key="gpustack_35c1d25d2f04a28f_900dccd6cf7d1b5fd4bc99725ce2bb0f",
	api_base="http://211.90.240.240:30001/v1-openai/chat/completions",
	top_k=1,
	temperature=1.0,
	stop_words=['<|im_end|>'],
	max_new_tokens=1024,
)

system_prompt = "你的回答只能从“典”，“孝”，“急”三个字中选一个。"
agent = Agent(llm, system_prompt)

user_msg = AgentMessage(sender="user", content="今天的天气情况怎么样")
bot_msg = agent(user_msg)
print(bot_msg)
```

我们可以来玩一下 lagent 中的 llm 类，先来看看流式输出（不加 time.sleep 无法流式，很奇怪啊，可能是终端的问题？）

```python
from lagent.schema import AgentMessage
from lagent.llms import GPTAPI
import time

# 初始化 LLM
llm = GPTAPI(
    model_type="Qwen2.5-32B-Instruct",
    key="gpustack_35c1d25d2f04a28f_900dccd6cf7d1b5fd4bc99725ce2bb0f",
    api_base="http://211.90.240.240:30001/v1-openai/chat/completions",
    top_k=1,
    temperature=1.0,
    stop_words=['<|im_end|>'],
    max_new_tokens=1024,
)

# 你想要的 system_prompt
system_prompt = "你是一个哲学家。"

# 构造用户消息
user_msg = AgentMessage(sender="user", content="人生的意义是什么？")

# 拼接成 OpenAI 接口能吃的消息列表
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_msg.content}
]

from lagent.schema import ModelStatusCode

print("Bot: ", end="", flush=True)
for status, chunk, _ in llm.stream_chat(messages):
    if status == ModelStatusCode.STREAM_ING:
        print(chunk, end="", flush=True)
        time.sleep(0.01)
    elif status == ModelStatusCode.END:
        print()
```


更多详细的关于 lagent llm 类的解读看：[[GPTAPI 源码解读]]

### Memory as State


### Custom Message Aggregation

`DefaultAggregator` is called under the hood to assemble and convert `AgentMessage` to OpenAI message format.

```python
def forward(self, *message: AgentMessage, session_id=0, **kwargs) -> Union[AgentMessage, str]:
	formatted_messages = self.aggregator.aggregate(
		self.memory.get(session_id),
		self.name,
		self.output_format,
		self.template
	)
	llm_response = self.llm.chat(formatted_messages, **kwargs)
```

Implement a simple aggregator that can receive few-shots

```python
from typing import List, Union
from lagent.memory import Memory 
from lagent.prompts import StrParser
from lagent.agents.aggregator import DefaultAggregator

class FewshotAggregator(DefaultAggregator):
	def __init__(self, few_shot: List[dict] = None):
		self.few_shot = few_shot or []

	def aggregate(self,
	message: Memory, name: str, parser: StrParser = None, system_instruction: Union[str, dict, List[dict]] = None
	) -> List[dict]:
		_message = []
		if system_instruction:
			_message.extend(self.aggregate_system_instruction(system_instruction))
		_message.extend(self.few_shot)
		messages = messages.get_memory()
		for message in messages:
			if message.sender == name:
				_message.append(dict(role="assistant", content=str(message.content)))
			else:
				user_message = message.content
				if len(_message) > 0 and _message[-1]['role'] == 'user':
					_message[-1]['content'] += user_message
				else:
					_message.append(dict(role='user', content=user_message))
		return _message

agent = Agent(
	llm,
	aggregator=FewshotAggregator(
		[
			{"role": "user", "content": "今天天气"},
			{"role": "assistant", "content": "【晴】"},
		]
	)
)

user_msg = AgentMessage(sender='user', content='昨天天气')
bot_msg = agent(user_msg)
print(bot_msg)
```

### Flexible Response Formatting

In `AgentMessage` , `formatted` is reserved to store infromation parsed by `output_format` from the model output.

```python
def forward(self, *message: AgentMessage, session_id=0, **kwargs) -> Union[AgentMessage, str]:
	...
	llm_response = self.llm_chat(formatted_messages, **kwargs)
	if self.output_format:
		formatted_message = self.output_format.parse_response(llm_response)
		return AgentMessage(
			sender=self.name,
			content=llm_response,
			formatted=formatted_message
		)
	...
```

Use a tool parser as follows

```python
from lagent.prompts.parsers import ToolParser

system_prompt = "逐步分析并编写 python 代码解决以下问题。"
parser = ToolParser(tool_type="code interpreter", begin="```python\n", end="\n```\n")
llm.gen_params["stop_words"].append('\n```\n')

user_msg = AgentMessage(
	sender="user",
	content='Marie is thinking '
)
bot_msg = agent(user_msg)
print(bot_msg.model_dump_json(indent=4))
```


### Dual interfaces


### Consistency of Tool Calling 

`ActionExecutor` uses the same communication data structure as Agent, but requires the content of input `AgentMessage` to be a dict containing:


## Practice

Try to implement `forward` instead of `__call__` of subclasses unless necessary. 

Always include the `session_id` argument explicitly, which is designed for isolation of memory, LLM requests and tool invocation (e.g. maintain multiple independent IPython environments) in concurrency.

### Single Agent

```python
from lagent.agents.aggregator import InternLMToolAggregator

class Coder(Agent):
	def __init__(self, model_path, system_prompt, max_turn=3):
		llm = VllmModel(
			path=model_path, 
			meta_template=INTERNLM2_META,
			tp=1,
			top_k=1,
			temperature=1.0,
			stop_words=['\n```\n', '<|im_end|>'],
			max_new_tokens=1024
		)
		self.agent = Agent(
			llm,
			system_prompt,
			output_format=ToolParser(
				tool_type="code interpreter", begin="```python\n", end="\n```\n"
			)
			aggreator=InternLMToolAggregator(),
		)
		self.executor = ActionExecutor([IPythonInteractive()], hooks=[CodeProcessor()])
		self.max_turn = max_turn

	def forward(self, message: AgentMessage, session_id=0) -> AgentMessage:
        for _ in range(self.max_turn):
            message = self.agent(message, session_id=session_id)
            if message.formatted['tool_type'] is None:
                return message
            message = self.executor(message, session_id=session_id)
        return message

coder = Coder('Qwen/Qwen2-7B-Instruct', 'Solve the problem step by step with assistance of Python code')
query = AgentMessage(
    sender='user',
    content='Find the projection of $\\mathbf{a}$ onto $\\mathbf{b} = '
    '\\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$ if $\\mathbf{a} \\cdot \\mathbf{b} = 2.$'
)
answer = coder(query)
print(answer.content)
print('-' * 120)
for msg in coder.state_dict()['agent.memory']:
    print('*' * 80)
    print(f'{msg["sender"]}:\n\n{msg["content"]}')
```

### Multiple Agents

Asynchronous blogging agents that improve writing quality by self-refinement.

```python
import asyncio
import os
from lagent.llms import AsyncGPTAPI
from lagent.agents import AsyncAgent

os.environ['OPENAI_API_KEY'] = 'YOU_API_KEY'

class PrefixedMessageHook(Hook):
	def __init__(self, prefix: str, senders: list = None):
		self.prefix = prefix
		self.senders = senders or []

	def before_agent(self, agent, messages, session_id):
		for message in messages:
			if message.sender in self.senders:
				message.content = self.prefix + message.content

class AsyncBlogger(AsyncAgent):
	def __init__(self, model_path, writer_prompt, critic_prompt, critic_prefix='', max_turn=3):
		super().__init__()
		llm = AsyncGPTAPI(model_type=model_path, retry=5, max_new_token=2048)
		self.writer = AsyncAgent(llm, writer_prompt, name="writer")
		self.critic = AsyncAgent(
			llm, critic_prompt, name="critic", hooks=[PrefixedMessageHook(critic_prefix, ['writer'])]
		)
		self.max_turn = max_turn

	async def forward(self, message: AgentMessage, session_id=0):
		for _ in range(self.max_turn):
			message = await self.writer(message, session_id=session_id)
			message = await self.critic(message, session_id=session_id)
		return await self.writer(message, session_id=session_id)

blogger = AsyncBlogger(
	'gpt-4o-2024-05-13',
	writer_prompt="You are an writing assistant tasked to write engaging blogpost. You try to generate the best possible for the user's request."
	"If the user provides critique, then respond with a revised version of your previous attempts",
	critic_prompt="Generate critique and recommendations on the writing. Provide detailed recommendations, including requests for length, depth, style, etc...",
	critic_prefix='Reflect and provide critique on the following writing. \n\n'
)

user_prompt = (
	"Write an enagaging blogpost on the recent updates in {topic}. "
	"The blogpost should be engaging and understandable for general audience. "
	"Should have more than 3 paragraphes but no longer than 1000 words."
)

bot_msgs = asyncio.get_event_loop().run_until_complete(
	asyncio.gathert(
		*[
			blogger(AgentMessage(sender="user", content=user_prompt.format(topic=topic)), session_id=i)
			for i, topic in enumerate(["AI", "Biotechnology", "New Energy", "Video Games", "Pop Music"])
		]
	)
)

print(bot_msgs[0].content)
print('-' * 120)
for msg in blogger.state_dict(session_id=0)['writer.memory']:
    print('*' * 80)
    print(f'{msg["sender"]}:\n\n{msg["content"]}')
print('-' * 120)
for msg in blogger.state_dict(session_id=0)['critic.memory']:
    print('*' * 80)
    print(f'{msg["sender"]}:\n\n{msg["content"]}')
```


A multi-agent workflow that performs information retrieval, data collection and chart plotting.

```python
import json
from lagent.actions import IPythonInterpreter, WebBrowser, ActionExecutor
from lagent.agents.stream import get_plugin_prompt
from lagent.llms import GPTAPI
from lagent.hooks import InternLMActionProcessor

TOOL_TEMPLATE = (
    "You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress"
    " towards answering the question. If you are unable to fully answer, that's OK, another assistant with"
    " different tools will help where you left off. Execute what you can to make progress. If you or any of"
    " the other assistants have the final answer or deliverable, prefix your response with {finish_pattern}"
    " so the team knows to stop. You have access to the following tools:\n{tool_description}\nPlease provide"
    " your thought process when you need to use a tool, followed by the call statement in this format:"
    "\n{invocation_format}\\\\n**{system_prompt}**"
)

class DataVisualizer(Agent):
    def __init__(self, model_path, research_prompt, chart_prompt, finish_pattern="Final Answer", max_turn=10):
        super().__init__()
        llm = GPTAPI(model_path, key='YOUR_OPENAI_API_KEY', retry=5, max_new_tokens=1024, stop_words=["```\n"])
        interpreter, browser = IPythonInterpreter(), WebBrowser("BingSearch", api_key="YOUR_BING_API_KEY")
        self.researcher = Agent(
            llm,
            TOOL_TEMPLATE.format(
                finish_pattern=finish_pattern,
                tool_description=get_plugin_prompt(browser),
                invocation_format='```json\n{"name": {{tool name}}, "parameters": {{keyword arguments}}}\n```\n',
                system_prompt=research_prompt,
            ),
            output_format=ToolParser(
                "browser",
                begin="```json\n",
                end="\n```\n",
                validate=lambda x: json.loads(x.rstrip('`')),
            ),
            aggregator=InternLMToolAggregator(),
            name="researcher",
        )
        self.charter = Agent(
            llm,
            TOOL_TEMPLATE.format(
                finish_pattern=finish_pattern,
                tool_description=interpreter.name,
                invocation_format='```python\n{{code}}\n```\n',
                system_prompt=chart_prompt,
            ),
            output_format=ToolParser(
                "interpreter",
                begin="```python\n",
                end="\n```\n",
                validate=lambda x: x.rstrip('`'),
            ),
            aggregator=InternLMToolAggregator(),
            name="charter",
        )
        self.executor = ActionExecutor([interpreter, browser], hooks=[InternLMActionProcessor()])
        self.finish_pattern = finish_pattern
        self.max_turn = max_turn

    def forward(self, message, session_id=0):
        for _ in range(self.max_turn):
            message = self.researcher(message, session_id=session_id, stop_words=["```\n", "```python"]) # override llm stop words
            while message.formatted["tool_type"]:
                message = self.executor(message, session_id=session_id)
                message = self.researcher(message, session_id=session_id, stop_words=["```\n", "```python"])
            if self.finish_pattern in message.content:
                return message
            message = self.charter(message)
            while message.formatted["tool_type"]:
                message = self.executor(message, session_id=session_id)
                message = self.charter(message, session_id=session_id)
            if self.finish_pattern in message.content:
                return message
        return message

visualizer = DataVisualizer(
    "gpt-4o-2024-05-13",
    research_prompt="You should provide accurate data for the chart generator to use.",
    chart_prompt="Any charts you display will be visible by the user.",
)
user_msg = AgentMessage(
    sender='user',
    content="Fetch the China's GDP over the past 5 years, then draw a line graph of it. Once you code it up, finish.")
bot_msg = visualizer(user_msg)
print(bot_msg.content)
json.dump(visualizer.state_dict(), open('visualizer.json', 'w'), ensure_ascii=False, indent=4)
```



